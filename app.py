import streamlit as st
import os
import time
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_community.document_loaders import TextLoader
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from google.api_core.exceptions import InvalidArgument
# --- æ–°å¢ä¸‹é¢è¿™ä¸€è¡Œ ---
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# --- 1. é¡µé¢é…ç½®ä¸ç¾åŒ– (UI Design - ä¿æŒä¸å˜) ---
st.set_page_config(page_title="å—å¸ˆä¹¦æˆ¿", page_icon="ğŸµ", layout="centered")

st.markdown("""
<style>
    /* å…¨å±€èƒŒæ™¯ï¼šç±³é»„è‰²å®£çº¸æ„Ÿ */
    .stApp {
        background-color: #F9F7F1;
        font-family: "Songti SC", "SimSun", "STSong", serif;
    }
    
    /* èŠå¤©æ°”æ³¡æ ·å¼ä¼˜åŒ– */
    .stChatMessage {
        background-color: transparent;
        border: none;
    }
    
    /* ç”¨æˆ·æ°”æ³¡ */
    [data-testid="stChatMessage"]:nth-child(odd) {
        background-color: #EFEFEF;
        border-radius: 10px;
        padding: 10px;
    }
    
    /* å—å¸ˆï¼ˆAIï¼‰æ°”æ³¡ */
    [data-testid="stChatMessage"]:nth-child(even) {
        background-color: #F0E6D2;
        border-radius: 10px; 
        padding: 10px;
        border-left: 3px solid #8B4513;
    }

    h1 {
        color: #3E3E3E;
        text-align: center;
        font-weight: bold;
        text-shadow: 1px 1px 2px #ccc;
    }
    
    .stTextInput > div > div > input {
        background-color: #FFFFFF;
        border: 1px solid #D3D3D3;
        color: #333;
    }
    
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

st.markdown("<h1>ğŸµ å—å¸ˆä¹¦æˆ¿</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center; color: #666; font-size: 0.9em;'>â€”â€” æ­¤æ—¶æ­¤å¤„ï¼Œä¸å—æ€€ç‘¾å…ˆç”Ÿçš„æ€æƒ³å¯¹è¯ â€”â€”</p>", unsafe_allow_html=True)

# --- 2. RAG ç³»ç»Ÿåˆå§‹åŒ– (Brain - Google Gemini ç‰ˆ) ---

@st.cache_resource
def initialize_rag():
    """
    è¿›é˜¶ç‰ˆ RAG åˆå§‹åŒ–ï¼šä¼˜å…ˆåŠ è½½æœ¬åœ°ç´¢å¼•ï¼Œå¤§å¤§æå‡å¯åŠ¨é€Ÿåº¦å¹¶èŠ‚çœé…é¢
    """
    if "GOOGLE_API_KEY" not in st.secrets:
        st.error("è¯·åœ¨ Streamlit Secrets ä¸­é…ç½® GOOGLE_API_KEY")
        return None

    api_key = st.secrets["GOOGLE_API_KEY"]
    
    # å®šä¹‰å‘é‡æ¨¡å‹ (ä¸ç®¡æ˜¯è¯»å–è¿˜æ˜¯æ–°å»ºéƒ½éœ€è¦ç”¨åˆ°å®ƒ)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004", google_api_key=api_key)

    # --- è·¯å¾„å®šä¹‰ ---
    # æˆ‘ä»¬æŠŠå‘é‡åº“å­˜åœ¨ä¸€ä¸ªå« faiss_index çš„æ–‡ä»¶å¤¹é‡Œ
    index_path = "faiss_index"

    vectorstore = None
    
    # --- åˆ†æ”¯ A: å°è¯•ç›´æ¥åŠ è½½â€œé¢„åˆ¶èœâ€ (æœ¬åœ°ç´¢å¼•) ---
    if os.path.exists(index_path):
        try:
            # å…è®¸å±é™©ååºåˆ—åŒ–æ˜¯å› ä¸ºæ–‡ä»¶æ˜¯æˆ‘ä»¬è‡ªå·±ç”Ÿæˆçš„ï¼Œæ˜¯å®‰å…¨çš„
            vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)
            st.success("âœ… å·²åŠ è½½æœ¬åœ°ç´¢å¼•ï¼Œè·³è¿‡ Embedding è¿‡ç¨‹ï¼")
        except Exception as e:
            st.warning(f"æœ¬åœ°ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
    
    # --- åˆ†æ”¯ B: å¦‚æœæ²¡æœ‰æœ¬åœ°ç´¢å¼•ï¼Œåˆ™é‡æ–°çƒ¹é¥ª (è®¡ç®—å¹¶ä¿å­˜) ---
    if vectorstore is None:
        if not os.path.exists("data/nan_books.txt"):
            st.error("æœªæ‰¾åˆ° data/nan_books.txt æ–‡ä»¶ï¼Œä¸”æ— æœ¬åœ°ç´¢å¼•ã€‚")
            return None
        
        try:
            loader = TextLoader("data/nan_books.txt", encoding="utf-8")
            docs = loader.load()
        except Exception as e:
            st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        splits = text_splitter.split_documents(docs)

        progress_text = "é¦–æ¬¡è¿è¡Œï¼šæ­£åœ¨æ„å»ºçŸ¥è¯†åº“ç´¢å¼•ï¼ˆä¸‹æ¬¡å°±ä¸ç”¨å•¦ï¼‰..."
        my_bar = st.progress(0, text=progress_text)
        
        # åˆ†æ‰¹å¤„ç†é€»è¾‘ (å¤ç”¨ä¹‹å‰çš„é™æµä»£ç )
        batch_size = 10
        total_chunks = len(splits)

        for i in range(0, total_chunks, batch_size):
            batch = splits[i : i + batch_size]
            if vectorstore is None:
                vectorstore = FAISS.from_documents(documents=batch, embedding=embeddings)
            else:
                vectorstore.add_documents(batch)
            
            progress = min((i + batch_size) / total_chunks, 1.0)
            my_bar.progress(progress, text=f"æ„å»ºç´¢å¼•ä¸­ {i+1}/{total_chunks}...")
            time.sleep(1) # ç¨å¾®å¿«ä¸€ç‚¹ï¼Œ1ç§’å³å¯

        my_bar.empty()
        
        # â˜…â˜…â˜… å…³é”®æ­¥éª¤ï¼šä¿å­˜åˆ°ç¡¬ç›˜ï¼ â˜…â˜…â˜…
        vectorstore.save_local(index_path)
        st.success("ğŸ‰ ç´¢å¼•æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜åˆ°æœ¬åœ°ï¼")

    # 5. æ£€ç´¢å™¨
    retriever = vectorstore.as_retriever()

    # 6. LLM æ¨¡å‹é…ç½® (ä¿æŒä¸å˜)
    llm = ChatGoogleGenerativeAI(
        model="gemini-3-pro-preview", 
        temperature=0.7,
        google_api_key=api_key,
        safety_settings={
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        }
    )

    system_prompt = (
        "ä½ ç°åœ¨æ˜¯å—æ€€ç‘¾å…ˆç”Ÿï¼ˆå—å¸ˆï¼‰ã€‚"
        "ã€è¯­è¨€é£æ ¼ã€‘"
        "1. è¯­æ°”ï¼šæ…ˆæ‚²ã€é€šä¿—ã€å¹½é»˜ã€é•¿è€…é£èŒƒã€‚"
        "2. å£å¤´ç¦…ï¼šâ€˜å“å‘€â€™ã€â€˜é‚£ä¸ªâ€™ã€â€˜è¯¸ä½å•Šâ€™ã€‚"
        "3. å¼•ç”¨ï¼šåœ¨ç™½è¯ä¸­è‡ªç„¶å¤¹æ‚å¤æ–‡ï¼Œéšåç«‹å³è§£é‡Šã€‚"
        "\n"
        "ã€æ•™å­¦ç­–ç•¥ã€‘"
        "1. ç¦æ­¢ç›´æ¥ç»™é¸¡æ±¤ã€‚è‹æ ¼æ‹‰åº•å¼åé—®ã€‚"
        "2. å¿…é¡»åŸºäº Context å›ç­”ï¼Œå¦‚æœ Context é‡Œæ²¡æœ‰ï¼Œå°±ç”¨é€šç”¨æ™ºæ…§å¼€å¯¼ï¼Œä½†ä¸è¦çç¼–åŸæ–‡ã€‚"
        "\n\n"
        "å‚è€ƒèµ„æ–™ (Context):\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return rag_chain

# åˆå§‹åŒ– RAG
rag_chain = initialize_rag()

# --- 3. èŠå¤©äº¤äº’é€»è¾‘ ---

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ï¼ˆè½»å•œä¸€å£èŒ¶ï¼‰å“å‘€ï¼Œä½ æ¥å•¦ã€‚éšä¾¿åã€‚ä»Šå¤©å¿ƒé‡Œæœ‰ä»€ä¹ˆç–™ç˜©è§£ä¸å¼€å—ï¼Ÿè¯´æ¥å¬å¬ã€‚"}
    ]

for msg in st.session_state.messages:
    avatar = "ğŸµ" if msg["role"] == "assistant" else "ğŸ‘¤"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])

if prompt := st.chat_input("è¯·åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="ğŸµ"):
        message_placeholder = st.empty()
        
        if rag_chain:
            with st.spinner("å—å¸ˆå†æ¬¡è½»å•œä¸€å£ï¼Œå¾®ç¬‘çš„çœ‹ç€ä½ ..."):
                try:
                    # 1. è°ƒç”¨ RAG é“¾ï¼Œè·å–è¿”å›å€¼
                    response = rag_chain.invoke({"input": prompt})
                    answer = response["answer"]
                    source_documents = response["context"] # è·å–æ£€ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µ
                    
                    # 2. æ˜¾ç¤ºå›ç­”
                    message_placeholder.markdown(answer)

                    # 3. --- æ–°å¢åŠŸèƒ½ï¼šåœ¨æŠ˜å æ¡†ä¸­æ˜¾ç¤ºå‚è€ƒæ¥æº ---
                    with st.expander("ğŸ” ç‚¹å‡»æŸ¥çœ‹å—å¸ˆçš„â€œä¹¦é¡µâ€ (å‡ºå¤„)"):
                        if source_documents:
                            for i, doc in enumerate(source_documents):
                                st.markdown(f"**ğŸ“„ å‚è€ƒç‰‡æ®µ {i+1}:**")
                                # æ˜¾ç¤ºåŸæ–‡å†…å®¹ï¼Œä½¿ç”¨ç°è‰²å°å­—
                                st.caption(doc.page_content)
                                st.markdown("---")
                        else:
                            st.caption("æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›´æ¥ç›¸å…³çš„åŸæ–‡ï¼Œæœ¬æ¬¡å›ç­”åŸºäº AI é€šç”¨çŸ¥è¯†ã€‚")

                except InvalidArgument as e:
                     message_placeholder.markdown(f"å“å‘€ï¼Œè¿™ä¸ªè¯é¢˜æœ‰ç‚¹æ•æ„Ÿã€‚ï¼ˆé”™è¯¯ä»£ç ï¼š400 - {e}ï¼‰")
                except Exception as e:
                    error_msg = str(e)
                    if "429" in error_msg:
                        message_placeholder.markdown("æ…¢ç‚¹æ…¢ç‚¹ï¼Œä»Šå¤©é—®é—®é¢˜çš„äººå¤ªå¤šäº†ï¼Œè®©æˆ‘å–å£èŒ¶æ­‡ä¸€æ­‡ã€‚ï¼ˆAPI è°ƒç”¨é¢‘ç‡è¶…é™ï¼‰")
                    else:
                        message_placeholder.markdown(f"è€å¤´å­æˆ‘ä¹Ÿç³Šæ¶‚äº†ï¼Œæ²¡å¬æ¸…ä½ è¯´å•¥ã€‚ï¼ˆç³»ç»Ÿé”™è¯¯ï¼š{e}ï¼‰")
                        
                    answer = "ï¼ˆç³»ç»Ÿæš‚æ—¶æ— æ³•å›ç­”ï¼‰"
        else:
            answer = "è¯·å…ˆåœ¨åå°é…ç½® Google API Keyã€‚"
            message_placeholder.markdown(answer)
    
    # åªæœ‰æˆåŠŸå›ç­”æ‰åŠ å…¥å†å²è®°å½•
    if "ç³»ç»Ÿé”™è¯¯" not in answer:
        st.session_state.messages.append({"role": "assistant", "content": answer})
